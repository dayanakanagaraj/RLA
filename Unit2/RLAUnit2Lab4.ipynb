{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f902a6-47c7-4ed7-b56f-12ef0d73ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cart Pole Balancing with Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b3b3fde-0493-4a58-b51c-863b72864da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "      --------------------------------------- 10.2/721.7 kB ? eta -:--:--\n",
      "     - ----------------------------------- 30.7/721.7 kB 435.7 kB/s eta 0:00:02\n",
      "     --- --------------------------------- 61.4/721.7 kB 656.4 kB/s eta 0:00:02\n",
      "     ---- -------------------------------- 92.2/721.7 kB 525.1 kB/s eta 0:00:02\n",
      "     ------ ----------------------------- 122.9/721.7 kB 554.9 kB/s eta 0:00:02\n",
      "     -------- --------------------------- 163.8/721.7 kB 701.4 kB/s eta 0:00:01\n",
      "     ---------- ------------------------- 204.8/721.7 kB 692.4 kB/s eta 0:00:01\n",
      "     ---------- ------------------------- 215.0/721.7 kB 624.4 kB/s eta 0:00:01\n",
      "     ----------- ------------------------ 235.5/721.7 kB 654.9 kB/s eta 0:00:01\n",
      "     ------------- ---------------------- 266.2/721.7 kB 630.5 kB/s eta 0:00:01\n",
      "     -------------- --------------------- 286.7/721.7 kB 632.7 kB/s eta 0:00:01\n",
      "     ---------------- ------------------- 327.7/721.7 kB 634.9 kB/s eta 0:00:01\n",
      "     ------------------ ----------------- 368.6/721.7 kB 655.2 kB/s eta 0:00:01\n",
      "     ------------------- ---------------- 399.4/721.7 kB 655.1 kB/s eta 0:00:01\n",
      "     -------------------- --------------- 409.6/721.7 kB 623.0 kB/s eta 0:00:01\n",
      "     --------------------- -------------- 430.1/721.7 kB 624.8 kB/s eta 0:00:01\n",
      "     ---------------------- ------------- 450.6/721.7 kB 612.4 kB/s eta 0:00:01\n",
      "     ---------------------- ------------- 460.8/721.7 kB 600.7 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 481.3/721.7 kB 591.0 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 481.3/721.7 kB 591.0 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 481.3/721.7 kB 591.0 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 481.3/721.7 kB 591.0 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 481.3/721.7 kB 591.0 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 481.3/721.7 kB 591.0 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 481.3/721.7 kB 591.0 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 481.3/721.7 kB 591.0 kB/s eta 0:00:01\n",
      "     ------------------------------- ---- 624.6/721.7 kB 524.2 kB/s eta 0:00:01\n",
      "     ------------------------------- ---- 624.6/721.7 kB 524.2 kB/s eta 0:00:01\n",
      "     -------------------------------- --- 645.1/721.7 kB 507.9 kB/s eta 0:00:01\n",
      "     -------------------------------- --- 645.1/721.7 kB 507.9 kB/s eta 0:00:01\n",
      "     -------------------------------- --- 655.4/721.7 kB 485.8 kB/s eta 0:00:01\n",
      "     -------------------------------- --- 655.4/721.7 kB 485.8 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 675.8/721.7 kB 468.1 kB/s eta 0:00:01\n",
      "     ---------------------------------- - 696.3/721.7 kB 472.2 kB/s eta 0:00:01\n",
      "     ---------------------------------- - 696.3/721.7 kB 472.2 kB/s eta 0:00:01\n",
      "     ---------------------------------- - 696.3/721.7 kB 472.2 kB/s eta 0:00:01\n",
      "     -----------------------------------  716.8/721.7 kB 439.1 kB/s eta 0:00:01\n",
      "     ------------------------------------ 721.7/721.7 kB 442.2 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.18.0 in e:\\anaconda\\lib\\site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in e:\\anaconda\\lib\\site-packages (from gym) (2.2.1)\n",
      "Collecting gym_notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827702 sha256=c24ffc50ccf6da64d8586e9a02bdafbef753e7504ebe656d732df7c72ea73a54\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\95\\51\\6c\\9bb05ebbe7c5cb8171dfaa3611f32622ca4658d53f31c79077\n",
      "Successfully built gym\n",
      "Installing collected packages: gym_notices, gym\n",
      "Successfully installed gym-0.26.2 gym_notices-0.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c636368d-1483-4d3a-ab11-cb74f9d946b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 17.0\n",
      "Episode 2: Total Reward = 30.0\n",
      "Episode 3: Total Reward = 21.0\n",
      "Episode 4: Total Reward = 15.0\n",
      "Episode 5: Total Reward = 30.0\n",
      "Episode 6: Total Reward = 24.0\n",
      "Episode 7: Total Reward = 15.0\n",
      "Episode 8: Total Reward = 16.0\n",
      "Episode 9: Total Reward = 14.0\n",
      "Episode 10: Total Reward = 10.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Number of episodes to run\n",
    "num_episodes = 10\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation, _ = env.reset()  # Reset the environment\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()  # Render the environment (optional)\n",
    "        action = env.action_space.sample()  # Choose a random action (0 or 1)\n",
    "        observation, reward, done, truncated, info = env.step(action)  # Take action\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c676b6-19e2-4d88-8c80-1051f1fa5f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unified Notation for Episodic and Continuing Tasks\n",
    "#the below The random policy performs poorly; proper RL training would improve performance.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f234da13-dc46-45e4-aca8-004048ace78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in e:\\anaconda\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy in e:\\anaconda\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in e:\\anaconda\\lib\\site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in e:\\anaconda\\lib\\site-packages (from gym) (0.0.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed4a8460-00db-4dd8-9ff5-4b5487a9dd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Episodic Task: CartPole-v1\n",
      "Episode 1: Total Reward = 26.0, Steps = 26, Discounted Return = 23.00\n",
      "Episode 2: Total Reward = 11.0, Steps = 11, Discounted Return = 10.47\n",
      "Episode 3: Total Reward = 11.0, Steps = 11, Discounted Return = 10.47\n",
      "Episode 4: Total Reward = 19.0, Steps = 19, Discounted Return = 17.38\n",
      "Episode 5: Total Reward = 28.0, Steps = 28, Discounted Return = 24.53\n",
      "\n",
      "Running Continuing Task: MountainCarContinuous-v0\n",
      "Episode 1: Total Reward = -33.06091481609596, Steps = 999, Discounted Return = -3.13\n",
      "Episode 2: Total Reward = -33.939005153118906, Steps = 999, Discounted Return = -3.27\n",
      "Episode 3: Total Reward = -33.339596986669726, Steps = 999, Discounted Return = -3.19\n",
      "Episode 4: Total Reward = -31.146623868199548, Steps = 999, Discounted Return = -2.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:211: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n",
      "E:\\Anaconda\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "E:\\Anaconda\\Lib\\site-packages\\gym\\envs\\classic_control\\continuous_mountain_car.py:193: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"MountainCarContinuous-v0\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5: Total Reward = -33.20020581169291, Steps = 999, Discounted Return = -3.34\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "def run_rl_task(env_name, gamma=0.99, num_episodes=10):\n",
    "    \"\"\"\n",
    "    Runs an RL environment using a random policy and calculates discounted return.\n",
    "    \n",
    "    Parameters:\n",
    "    - env_name: The Gym environment name (e.g., \"CartPole-v1\", \"MountainCarContinuous-v0\").\n",
    "    - gamma: Discount factor (0 â‰¤ Î³ â‰¤ 1).\n",
    "    - num_episodes: Number of episodes to run.\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    episodic = env.spec.max_episode_steps is not None  # Detect if task is episodic\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()  # Reset environment\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        rewards = []\n",
    "        step = 0\n",
    "\n",
    "        while not done:\n",
    "            env.render()  # Visualize environment (optional)\n",
    "            action = env.action_space.sample()  # Take random action\n",
    "            state, reward, done, truncated, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            total_reward += reward\n",
    "            step += 1\n",
    "\n",
    "            if episodic and (done or truncated):  # End if episodic\n",
    "                break\n",
    "\n",
    "        # Compute discounted return\n",
    "        G = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            G = rewards[t] + gamma * G\n",
    "\n",
    "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}, Steps = {step}, Discounted Return = {G:.2f}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "# Run episodic task (CartPole)\n",
    "print(\"\\nRunning Episodic Task: CartPole-v1\")\n",
    "run_rl_task(\"CartPole-v1\", gamma=0.99, num_episodes=5)\n",
    "\n",
    "# Run continuing task (MountainCarContinuous)\n",
    "print(\"\\nRunning Continuing Task: MountainCarContinuous-v0\")\n",
    "run_rl_task(\"MountainCarContinuous-v0\", gamma=0.99, num_episodes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffffeccb-5b8f-48f3-a56e-f4666bc9b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Policies and Value Functions\n",
    "#The optimal policy guides the agent toward the goal.\n",
    "#The value function estimates the expected return from each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d834114-dbf5-47dc-a6c5-c8bec994db1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Running Policy Iteration...\n",
      "Optimal Policy (Policy Iteration):\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Optimal Value Function:\n",
      "[0.54201383 0.49878715 0.47067694 0.45683158 0.55844021 0.\n",
      " 0.35833998 0.         0.59178998 0.64307352 0.61520205 0.\n",
      " 0.         0.7417161  0.86283524 0.        ]\n",
      "\n",
      "ðŸ”¹ Running Value Iteration...\n",
      "Optimal Policy (Value Iteration):\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Optimal Value Function:\n",
      "[0.54201404 0.49878743 0.47067727 0.45683193 0.5584404  0.\n",
      " 0.35834012 0.         0.59179013 0.64307363 0.61520214 0.\n",
      " 0.         0.74171617 0.86283528 0.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def evaluate_policy(env, policy, gamma=0.99, theta=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the state-value function V(s) for a given policy using iterative policy evaluation.\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.observation_space.n)  # Initialize value function\n",
    "    while True:\n",
    "        delta = 0  # Track convergence\n",
    "        for s in range(env.observation_space.n):\n",
    "            v = 0\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    v += action_prob * prob * (reward + gamma * V[next_state] * (not done))\n",
    "            delta = max(delta, abs(V[s] - v))\n",
    "            V[s] = v\n",
    "        if delta < theta:  # Stop when values converge\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def improve_policy(env, V, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Computes a new greedy policy using the updated value function V(s).\n",
    "    \"\"\"\n",
    "    policy = np.zeros((env.observation_space.n, env.action_space.n))  # Initialize new policy\n",
    "    for s in range(env.observation_space.n):\n",
    "        q_values = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                q_values[a] += prob * (reward + gamma * V[next_state] * (not done))\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[s] = np.eye(env.action_space.n)[best_action]  # One-hot encoding\n",
    "    return policy\n",
    "\n",
    "def policy_iteration(env, gamma=0.99, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Policy Iteration: Alternates between policy evaluation and improvement.\n",
    "    \"\"\"\n",
    "    policy = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n  # Initialize random policy\n",
    "    for i in range(max_iterations):\n",
    "        V = evaluate_policy(env, policy, gamma)\n",
    "        new_policy = improve_policy(env, V, gamma)\n",
    "        if np.all(policy == new_policy):  # Stop if policy is stable\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy, V\n",
    "\n",
    "def value_iteration(env, gamma=0.99, theta=1e-6):\n",
    "    \"\"\"\n",
    "    Value Iteration: Computes the optimal value function and policy.\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.observation_space.n)  # Initialize values\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            q_values = np.zeros(env.action_space.n)\n",
    "            for a in range(env.action_space.n):\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    q_values[a] += prob * (reward + gamma * V[next_state] * (not done))\n",
    "            max_q = np.max(q_values)\n",
    "            delta = max(delta, abs(V[s] - max_q))\n",
    "            V[s] = max_q\n",
    "        if delta < theta:  # Stop if values converge\n",
    "            break\n",
    "\n",
    "    # Derive optimal policy from the optimal value function\n",
    "    policy = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for s in range(env.observation_space.n):\n",
    "        q_values = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                q_values[a] += prob * (reward + gamma * V[next_state] * (not done))\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[s] = np.eye(env.action_space.n)[best_action]  # One-hot encoding\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "# Initialize FrozenLake environment\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "# Perform Policy Iteration\n",
    "print(\"\\nðŸ”¹ Running Policy Iteration...\")\n",
    "optimal_policy_pi, optimal_value_pi = policy_iteration(env)\n",
    "print(\"Optimal Policy (Policy Iteration):\")\n",
    "print(optimal_policy_pi)\n",
    "print(\"Optimal Value Function:\")\n",
    "print(optimal_value_pi)\n",
    "\n",
    "# Perform Value Iteration\n",
    "print(\"\\nðŸ”¹ Running Value Iteration...\")\n",
    "optimal_policy_vi, optimal_value_vi = value_iteration(env)\n",
    "print(\"Optimal Policy (Value Iteration):\")\n",
    "print(optimal_policy_vi)\n",
    "print(\"Optimal Value Function:\")\n",
    "print(optimal_value_vi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f01686-7ee2-4357-bf40-60beac8aefde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimal Policies and Optimal Value Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1527d31b-9376-41eb-a684-974664471316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Running Value Iteration...\n",
      "Optimal Policy:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Optimal Value Function:\n",
      "[0.54201404 0.49878743 0.47067727 0.45683193 0.5584404  0.\n",
      " 0.35834012 0.         0.59179013 0.64307363 0.61520214 0.\n",
      " 0.         0.74171617 0.86283528 0.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "def value_iteration(env, gamma=0.99, theta=1e-6):\n",
    "    \"\"\"\n",
    "    Performs Value Iteration to find the optimal value function and policy.\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.observation_space.n)  # Initialize value function\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            q_values = np.zeros(env.action_space.n)\n",
    "            for a in range(env.action_space.n):\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    q_values[a] += prob * (reward + gamma * V[next_state] * (not done))\n",
    "            max_q = np.max(q_values)\n",
    "            delta = max(delta, abs(V[s] - max_q))\n",
    "            V[s] = max_q  # Update value function\n",
    "\n",
    "        if delta < theta:  # Convergence check\n",
    "            break\n",
    "\n",
    "    # Derive optimal policy from the optimal value function\n",
    "    policy = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for s in range(env.observation_space.n):\n",
    "        q_values = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                q_values[a] += prob * (reward + gamma * V[next_state] * (not done))\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[s] = np.eye(env.action_space.n)[best_action]  # One-hot encoding\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "# Initialize FrozenLake environment\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "# Perform Value Iteration\n",
    "print(\"\\nðŸ”¹ Running Value Iteration...\")\n",
    "optimal_policy, optimal_value = value_iteration(env)\n",
    "\n",
    "print(\"Optimal Policy:\")\n",
    "print(optimal_policy)\n",
    "print(\"Optimal Value Function:\")\n",
    "print(optimal_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9cbdbe-c368-471a-8e1a-9b4ba85d572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimality and Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11754884-c08d-4de2-a54c-470137b8cbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in e:\\anaconda\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy in e:\\anaconda\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in e:\\anaconda\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in e:\\anaconda\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in e:\\anaconda\\lib\\site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in e:\\anaconda\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: filelock in e:\\anaconda\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in e:\\anaconda\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in e:\\anaconda\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in e:\\anaconda\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in e:\\anaconda\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in e:\\anaconda\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in e:\\anaconda\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\anaconda\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\anaconda\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\anaconda\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym numpy torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9897f1-07ba-45ca-bd8c-c2d65167be1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5056\\1931862767.py:49: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  states = torch.FloatTensor(states)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 23.0\n",
      "Episode 2: Reward = 9.0\n",
      "Episode 3: Reward = 13.0\n",
      "Episode 4: Reward = 17.0\n",
      "Episode 5: Reward = 25.0\n",
      "Episode 6: Reward = 18.0\n",
      "Episode 7: Reward = 17.0\n",
      "Episode 8: Reward = 15.0\n",
      "Episode 9: Reward = 17.0\n",
      "Episode 10: Reward = 33.0\n",
      "Episode 11: Reward = 39.0\n",
      "Episode 12: Reward = 31.0\n",
      "Episode 13: Reward = 20.0\n",
      "Episode 14: Reward = 10.0\n",
      "Episode 15: Reward = 42.0\n",
      "Episode 16: Reward = 21.0\n",
      "Episode 17: Reward = 15.0\n",
      "Episode 18: Reward = 52.0\n",
      "Episode 19: Reward = 13.0\n",
      "Episode 20: Reward = 30.0\n",
      "Episode 21: Reward = 40.0\n",
      "Episode 22: Reward = 19.0\n",
      "Episode 23: Reward = 20.0\n",
      "Episode 24: Reward = 15.0\n",
      "Episode 25: Reward = 15.0\n",
      "Episode 26: Reward = 16.0\n",
      "Episode 27: Reward = 23.0\n",
      "Episode 28: Reward = 18.0\n",
      "Episode 29: Reward = 14.0\n",
      "Episode 30: Reward = 12.0\n",
      "Episode 31: Reward = 12.0\n",
      "Episode 32: Reward = 11.0\n",
      "Episode 33: Reward = 18.0\n",
      "Episode 34: Reward = 29.0\n",
      "Episode 35: Reward = 19.0\n",
      "Episode 36: Reward = 14.0\n",
      "Episode 37: Reward = 38.0\n",
      "Episode 38: Reward = 10.0\n",
      "Episode 39: Reward = 16.0\n",
      "Episode 40: Reward = 9.0\n",
      "Episode 41: Reward = 19.0\n",
      "Episode 42: Reward = 17.0\n",
      "Episode 43: Reward = 14.0\n",
      "Episode 44: Reward = 12.0\n",
      "Episode 45: Reward = 29.0\n",
      "Episode 46: Reward = 59.0\n",
      "Episode 47: Reward = 14.0\n",
      "Episode 48: Reward = 18.0\n",
      "Episode 49: Reward = 18.0\n",
      "Episode 50: Reward = 31.0\n",
      "Episode 51: Reward = 38.0\n",
      "Episode 52: Reward = 18.0\n",
      "Episode 53: Reward = 14.0\n",
      "Episode 54: Reward = 14.0\n",
      "Episode 55: Reward = 15.0\n",
      "Episode 56: Reward = 15.0\n",
      "Episode 57: Reward = 12.0\n",
      "Episode 58: Reward = 27.0\n",
      "Episode 59: Reward = 15.0\n",
      "Episode 60: Reward = 11.0\n",
      "Episode 61: Reward = 16.0\n",
      "Episode 62: Reward = 10.0\n",
      "Episode 63: Reward = 15.0\n",
      "Episode 64: Reward = 11.0\n",
      "Episode 65: Reward = 26.0\n",
      "Episode 66: Reward = 25.0\n",
      "Episode 67: Reward = 15.0\n",
      "Episode 68: Reward = 10.0\n",
      "Episode 69: Reward = 27.0\n",
      "Episode 70: Reward = 11.0\n",
      "Episode 71: Reward = 26.0\n",
      "Episode 72: Reward = 14.0\n",
      "Episode 73: Reward = 11.0\n",
      "Episode 74: Reward = 12.0\n",
      "Episode 75: Reward = 9.0\n",
      "Episode 76: Reward = 11.0\n",
      "Episode 77: Reward = 11.0\n",
      "Episode 78: Reward = 10.0\n",
      "Episode 79: Reward = 13.0\n",
      "Episode 80: Reward = 17.0\n",
      "Episode 81: Reward = 14.0\n",
      "Episode 82: Reward = 15.0\n",
      "Episode 83: Reward = 18.0\n",
      "Episode 84: Reward = 20.0\n",
      "Episode 85: Reward = 18.0\n",
      "Episode 86: Reward = 39.0\n",
      "Episode 87: Reward = 19.0\n",
      "Episode 88: Reward = 33.0\n",
      "Episode 89: Reward = 37.0\n",
      "Episode 90: Reward = 36.0\n",
      "Episode 91: Reward = 21.0\n",
      "Episode 92: Reward = 68.0\n",
      "Episode 93: Reward = 24.0\n",
      "Episode 94: Reward = 70.0\n",
      "Episode 95: Reward = 14.0\n",
      "Episode 96: Reward = 37.0\n",
      "Episode 97: Reward = 18.0\n",
      "Episode 98: Reward = 69.0\n",
      "Episode 99: Reward = 9.0\n",
      "Episode 100: Reward = 32.0\n",
      "Episode 101: Reward = 126.0\n",
      "Episode 102: Reward = 22.0\n",
      "Episode 103: Reward = 21.0\n",
      "Episode 104: Reward = 41.0\n",
      "Episode 105: Reward = 15.0\n",
      "Episode 106: Reward = 52.0\n",
      "Episode 107: Reward = 55.0\n",
      "Episode 108: Reward = 18.0\n",
      "Episode 109: Reward = 16.0\n",
      "Episode 110: Reward = 19.0\n",
      "Episode 111: Reward = 29.0\n",
      "Episode 112: Reward = 36.0\n",
      "Episode 113: Reward = 23.0\n",
      "Episode 114: Reward = 28.0\n",
      "Episode 115: Reward = 37.0\n",
      "Episode 116: Reward = 32.0\n",
      "Episode 117: Reward = 34.0\n",
      "Episode 118: Reward = 64.0\n",
      "Episode 119: Reward = 49.0\n",
      "Episode 120: Reward = 45.0\n",
      "Episode 121: Reward = 80.0\n",
      "Episode 122: Reward = 53.0\n",
      "Episode 123: Reward = 77.0\n",
      "Episode 124: Reward = 34.0\n",
      "Episode 125: Reward = 33.0\n",
      "Episode 126: Reward = 52.0\n",
      "Episode 127: Reward = 20.0\n",
      "Episode 128: Reward = 67.0\n",
      "Episode 129: Reward = 39.0\n",
      "Episode 130: Reward = 34.0\n",
      "Episode 131: Reward = 57.0\n",
      "Episode 132: Reward = 31.0\n",
      "Episode 133: Reward = 55.0\n",
      "Episode 134: Reward = 67.0\n",
      "Episode 135: Reward = 16.0\n",
      "Episode 136: Reward = 78.0\n",
      "Episode 137: Reward = 78.0\n",
      "Episode 138: Reward = 40.0\n",
      "Episode 139: Reward = 59.0\n",
      "Episode 140: Reward = 61.0\n",
      "Episode 141: Reward = 29.0\n",
      "Episode 142: Reward = 50.0\n",
      "Episode 143: Reward = 28.0\n",
      "Episode 144: Reward = 38.0\n",
      "Episode 145: Reward = 48.0\n",
      "Episode 146: Reward = 21.0\n",
      "Episode 147: Reward = 46.0\n",
      "Episode 148: Reward = 20.0\n",
      "Episode 149: Reward = 28.0\n",
      "Episode 150: Reward = 41.0\n",
      "Episode 151: Reward = 29.0\n",
      "Episode 152: Reward = 85.0\n",
      "Episode 153: Reward = 51.0\n",
      "Episode 154: Reward = 118.0\n",
      "Episode 155: Reward = 33.0\n",
      "Episode 156: Reward = 33.0\n",
      "Episode 157: Reward = 96.0\n",
      "Episode 158: Reward = 171.0\n",
      "Episode 159: Reward = 65.0\n",
      "Episode 160: Reward = 42.0\n",
      "Episode 161: Reward = 43.0\n",
      "Episode 162: Reward = 104.0\n",
      "Episode 163: Reward = 12.0\n",
      "Episode 164: Reward = 50.0\n",
      "Episode 165: Reward = 39.0\n",
      "Episode 166: Reward = 57.0\n",
      "Episode 167: Reward = 22.0\n",
      "Episode 168: Reward = 36.0\n",
      "Episode 169: Reward = 36.0\n",
      "Episode 170: Reward = 94.0\n",
      "Episode 171: Reward = 29.0\n",
      "Episode 172: Reward = 53.0\n",
      "Episode 173: Reward = 136.0\n",
      "Episode 174: Reward = 41.0\n",
      "Episode 175: Reward = 54.0\n",
      "Episode 176: Reward = 31.0\n",
      "Episode 177: Reward = 49.0\n",
      "Episode 178: Reward = 65.0\n",
      "Episode 179: Reward = 47.0\n",
      "Episode 180: Reward = 56.0\n",
      "Episode 181: Reward = 101.0\n",
      "Episode 182: Reward = 96.0\n",
      "Episode 183: Reward = 87.0\n",
      "Episode 184: Reward = 45.0\n",
      "Episode 185: Reward = 104.0\n",
      "Episode 186: Reward = 77.0\n",
      "Episode 187: Reward = 52.0\n",
      "Episode 188: Reward = 40.0\n",
      "Episode 189: Reward = 172.0\n",
      "Episode 190: Reward = 54.0\n",
      "Episode 191: Reward = 84.0\n",
      "Episode 192: Reward = 84.0\n",
      "Episode 193: Reward = 115.0\n",
      "Episode 194: Reward = 30.0\n",
      "Episode 195: Reward = 54.0\n",
      "Episode 196: Reward = 65.0\n",
      "Episode 197: Reward = 65.0\n",
      "Episode 198: Reward = 94.0\n",
      "Episode 199: Reward = 74.0\n",
      "Episode 200: Reward = 79.0\n",
      "Episode 201: Reward = 57.0\n",
      "Episode 202: Reward = 36.0\n",
      "Episode 203: Reward = 55.0\n",
      "Episode 204: Reward = 92.0\n",
      "Episode 205: Reward = 71.0\n",
      "Episode 206: Reward = 166.0\n",
      "Episode 207: Reward = 64.0\n",
      "Episode 208: Reward = 57.0\n",
      "Episode 209: Reward = 135.0\n",
      "Episode 210: Reward = 122.0\n",
      "Episode 211: Reward = 107.0\n",
      "Episode 212: Reward = 118.0\n",
      "Episode 213: Reward = 101.0\n",
      "Episode 214: Reward = 84.0\n",
      "Episode 215: Reward = 54.0\n",
      "Episode 216: Reward = 125.0\n",
      "Episode 217: Reward = 172.0\n",
      "Episode 218: Reward = 48.0\n",
      "Episode 219: Reward = 110.0\n",
      "Episode 220: Reward = 62.0\n",
      "Episode 221: Reward = 70.0\n",
      "Episode 222: Reward = 75.0\n",
      "Episode 223: Reward = 71.0\n",
      "Episode 224: Reward = 167.0\n",
      "Episode 225: Reward = 161.0\n",
      "Episode 226: Reward = 196.0\n",
      "Episode 227: Reward = 126.0\n",
      "Episode 228: Reward = 120.0\n",
      "Episode 229: Reward = 167.0\n",
      "Episode 230: Reward = 241.0\n",
      "Episode 231: Reward = 263.0\n",
      "Episode 232: Reward = 176.0\n",
      "Episode 233: Reward = 168.0\n",
      "Episode 234: Reward = 281.0\n",
      "Episode 235: Reward = 196.0\n",
      "Episode 236: Reward = 340.0\n",
      "Episode 237: Reward = 189.0\n",
      "Episode 238: Reward = 171.0\n",
      "Episode 239: Reward = 260.0\n",
      "Episode 240: Reward = 235.0\n",
      "Episode 241: Reward = 185.0\n",
      "Episode 242: Reward = 196.0\n",
      "Episode 243: Reward = 232.0\n",
      "Episode 244: Reward = 247.0\n",
      "Episode 245: Reward = 199.0\n",
      "Episode 246: Reward = 219.0\n",
      "Episode 247: Reward = 230.0\n",
      "Episode 248: Reward = 178.0\n",
      "Episode 249: Reward = 181.0\n",
      "Episode 250: Reward = 168.0\n",
      "Episode 251: Reward = 198.0\n",
      "Episode 252: Reward = 202.0\n",
      "Episode 253: Reward = 240.0\n",
      "Episode 254: Reward = 207.0\n",
      "Episode 255: Reward = 224.0\n",
      "Episode 256: Reward = 316.0\n",
      "Episode 257: Reward = 274.0\n",
      "Episode 258: Reward = 206.0\n",
      "Episode 259: Reward = 259.0\n",
      "Episode 260: Reward = 143.0\n",
      "Episode 261: Reward = 173.0\n",
      "Episode 262: Reward = 292.0\n",
      "Episode 263: Reward = 250.0\n",
      "Episode 264: Reward = 208.0\n",
      "Episode 265: Reward = 314.0\n",
      "Episode 266: Reward = 236.0\n",
      "Episode 267: Reward = 273.0\n",
      "Episode 268: Reward = 213.0\n",
      "Episode 269: Reward = 249.0\n",
      "Episode 270: Reward = 256.0\n",
      "Episode 271: Reward = 256.0\n",
      "Episode 272: Reward = 305.0\n",
      "Episode 273: Reward = 484.0\n",
      "Episode 274: Reward = 275.0\n",
      "Episode 275: Reward = 522.0\n",
      "Episode 276: Reward = 431.0\n",
      "Episode 277: Reward = 236.0\n",
      "Episode 278: Reward = 15.0\n",
      "Episode 279: Reward = 278.0\n",
      "Episode 280: Reward = 85.0\n",
      "Episode 281: Reward = 576.0\n",
      "Episode 282: Reward = 236.0\n",
      "Episode 283: Reward = 147.0\n",
      "Episode 284: Reward = 239.0\n",
      "Episode 285: Reward = 199.0\n",
      "Episode 286: Reward = 485.0\n",
      "Episode 287: Reward = 334.0\n",
      "Episode 288: Reward = 285.0\n",
      "Episode 289: Reward = 420.0\n",
      "Episode 290: Reward = 146.0\n",
      "Episode 291: Reward = 428.0\n",
      "Episode 292: Reward = 145.0\n",
      "Episode 293: Reward = 222.0\n",
      "Episode 294: Reward = 218.0\n",
      "Episode 295: Reward = 445.0\n",
      "Episode 296: Reward = 228.0\n",
      "Episode 297: Reward = 219.0\n",
      "Episode 298: Reward = 268.0\n",
      "Episode 299: Reward = 279.0\n",
      "Episode 300: Reward = 295.0\n",
      "Episode 301: Reward = 220.0\n",
      "Episode 302: Reward = 240.0\n",
      "Episode 303: Reward = 242.0\n",
      "Episode 304: Reward = 278.0\n",
      "Episode 305: Reward = 209.0\n",
      "Episode 306: Reward = 243.0\n",
      "Episode 307: Reward = 216.0\n",
      "Episode 308: Reward = 297.0\n",
      "Episode 309: Reward = 248.0\n",
      "Episode 310: Reward = 326.0\n",
      "Episode 311: Reward = 265.0\n",
      "Episode 312: Reward = 350.0\n",
      "Episode 313: Reward = 275.0\n",
      "Episode 314: Reward = 243.0\n",
      "Episode 315: Reward = 235.0\n",
      "Episode 316: Reward = 297.0\n",
      "Episode 317: Reward = 339.0\n",
      "Episode 318: Reward = 42.0\n",
      "Episode 319: Reward = 9.0\n",
      "Episode 320: Reward = 12.0\n",
      "Episode 321: Reward = 9.0\n",
      "Episode 322: Reward = 11.0\n",
      "Episode 323: Reward = 9.0\n",
      "Episode 324: Reward = 9.0\n",
      "Episode 325: Reward = 8.0\n",
      "Episode 326: Reward = 11.0\n",
      "Episode 327: Reward = 11.0\n",
      "Episode 328: Reward = 10.0\n",
      "Episode 329: Reward = 11.0\n",
      "Episode 330: Reward = 8.0\n",
      "Episode 331: Reward = 11.0\n",
      "Episode 332: Reward = 10.0\n",
      "Episode 333: Reward = 13.0\n",
      "Episode 334: Reward = 12.0\n",
      "Episode 335: Reward = 12.0\n",
      "Episode 336: Reward = 19.0\n",
      "Episode 337: Reward = 10.0\n",
      "Episode 338: Reward = 13.0\n",
      "Episode 339: Reward = 13.0\n",
      "Episode 340: Reward = 9.0\n",
      "Episode 341: Reward = 9.0\n",
      "Episode 342: Reward = 10.0\n",
      "Episode 343: Reward = 11.0\n",
      "Episode 344: Reward = 10.0\n",
      "Episode 345: Reward = 9.0\n",
      "Episode 346: Reward = 9.0\n",
      "Episode 347: Reward = 9.0\n",
      "Episode 348: Reward = 11.0\n",
      "Episode 349: Reward = 9.0\n",
      "Episode 350: Reward = 10.0\n",
      "Episode 351: Reward = 11.0\n",
      "Episode 352: Reward = 10.0\n",
      "Episode 353: Reward = 12.0\n",
      "Episode 354: Reward = 10.0\n",
      "Episode 355: Reward = 12.0\n",
      "Episode 356: Reward = 10.0\n",
      "Episode 357: Reward = 9.0\n",
      "Episode 358: Reward = 10.0\n",
      "Episode 359: Reward = 8.0\n",
      "Episode 360: Reward = 10.0\n",
      "Episode 361: Reward = 10.0\n",
      "Episode 362: Reward = 9.0\n",
      "Episode 363: Reward = 10.0\n",
      "Episode 364: Reward = 10.0\n",
      "Episode 365: Reward = 11.0\n",
      "Episode 366: Reward = 9.0\n",
      "Episode 367: Reward = 18.0\n",
      "Episode 368: Reward = 12.0\n",
      "Episode 369: Reward = 8.0\n",
      "Episode 370: Reward = 10.0\n",
      "Episode 371: Reward = 10.0\n",
      "Episode 372: Reward = 11.0\n",
      "Episode 373: Reward = 12.0\n",
      "Episode 374: Reward = 10.0\n",
      "Episode 375: Reward = 17.0\n",
      "Episode 376: Reward = 63.0\n",
      "Episode 377: Reward = 21.0\n",
      "Episode 378: Reward = 71.0\n",
      "Episode 379: Reward = 129.0\n",
      "Episode 380: Reward = 216.0\n",
      "Episode 381: Reward = 132.0\n",
      "Episode 382: Reward = 110.0\n",
      "Episode 383: Reward = 114.0\n",
      "Episode 384: Reward = 136.0\n",
      "Episode 385: Reward = 110.0\n",
      "Episode 386: Reward = 133.0\n",
      "Episode 387: Reward = 129.0\n",
      "Episode 388: Reward = 108.0\n",
      "Episode 389: Reward = 117.0\n",
      "Episode 390: Reward = 130.0\n",
      "Episode 391: Reward = 140.0\n",
      "Episode 392: Reward = 123.0\n",
      "Episode 393: Reward = 101.0\n",
      "Episode 394: Reward = 145.0\n",
      "Episode 395: Reward = 195.0\n",
      "Episode 396: Reward = 149.0\n",
      "Episode 397: Reward = 132.0\n",
      "Episode 398: Reward = 387.0\n",
      "Episode 399: Reward = 254.0\n",
      "Episode 400: Reward = 282.0\n",
      "Episode 401: Reward = 283.0\n",
      "Episode 402: Reward = 248.0\n",
      "Episode 403: Reward = 263.0\n",
      "Episode 404: Reward = 226.0\n",
      "Episode 405: Reward = 269.0\n",
      "Episode 406: Reward = 316.0\n",
      "Episode 407: Reward = 277.0\n",
      "Episode 408: Reward = 231.0\n",
      "Episode 409: Reward = 279.0\n",
      "Episode 410: Reward = 231.0\n",
      "Episode 411: Reward = 333.0\n",
      "Episode 412: Reward = 267.0\n",
      "Episode 413: Reward = 388.0\n",
      "Episode 414: Reward = 307.0\n",
      "Episode 415: Reward = 303.0\n",
      "Episode 416: Reward = 194.0\n",
      "Episode 417: Reward = 167.0\n",
      "Episode 418: Reward = 272.0\n",
      "Episode 419: Reward = 193.0\n",
      "Episode 420: Reward = 143.0\n",
      "Episode 421: Reward = 195.0\n",
      "Episode 422: Reward = 229.0\n",
      "Episode 423: Reward = 163.0\n",
      "Episode 424: Reward = 274.0\n",
      "Episode 425: Reward = 173.0\n",
      "Episode 426: Reward = 134.0\n",
      "Episode 427: Reward = 158.0\n",
      "Episode 428: Reward = 216.0\n",
      "Episode 429: Reward = 167.0\n",
      "Episode 430: Reward = 181.0\n",
      "Episode 431: Reward = 166.0\n",
      "Episode 432: Reward = 177.0\n",
      "Episode 433: Reward = 173.0\n",
      "Episode 434: Reward = 219.0\n",
      "Episode 435: Reward = 222.0\n",
      "Episode 436: Reward = 188.0\n",
      "Episode 437: Reward = 197.0\n",
      "Episode 438: Reward = 284.0\n",
      "Episode 439: Reward = 277.0\n",
      "Episode 440: Reward = 219.0\n",
      "Episode 441: Reward = 268.0\n",
      "Episode 442: Reward = 181.0\n",
      "Episode 443: Reward = 260.0\n",
      "Episode 444: Reward = 218.0\n",
      "Episode 445: Reward = 202.0\n",
      "Episode 446: Reward = 316.0\n",
      "Episode 447: Reward = 244.0\n",
      "Episode 448: Reward = 225.0\n",
      "Episode 449: Reward = 399.0\n",
      "Episode 450: Reward = 270.0\n",
      "Episode 451: Reward = 321.0\n",
      "Episode 452: Reward = 156.0\n",
      "Episode 453: Reward = 344.0\n",
      "Episode 454: Reward = 239.0\n",
      "Episode 455: Reward = 203.0\n",
      "Episode 456: Reward = 177.0\n",
      "Episode 457: Reward = 236.0\n",
      "Episode 458: Reward = 174.0\n",
      "Episode 459: Reward = 163.0\n",
      "Episode 460: Reward = 11.0\n",
      "Episode 461: Reward = 8.0\n",
      "Episode 462: Reward = 27.0\n",
      "Episode 463: Reward = 9.0\n",
      "Episode 464: Reward = 47.0\n",
      "Episode 465: Reward = 208.0\n",
      "Episode 466: Reward = 220.0\n",
      "Episode 467: Reward = 240.0\n",
      "Episode 468: Reward = 334.0\n",
      "Episode 469: Reward = 193.0\n",
      "Episode 470: Reward = 265.0\n",
      "Episode 471: Reward = 193.0\n",
      "Episode 472: Reward = 180.0\n",
      "Episode 473: Reward = 186.0\n",
      "Episode 474: Reward = 223.0\n",
      "Episode 475: Reward = 251.0\n",
      "Episode 476: Reward = 184.0\n",
      "Episode 477: Reward = 205.0\n",
      "Episode 478: Reward = 215.0\n",
      "Episode 479: Reward = 371.0\n",
      "Episode 480: Reward = 203.0\n",
      "Episode 481: Reward = 190.0\n",
      "Episode 482: Reward = 330.0\n",
      "Episode 483: Reward = 265.0\n",
      "Episode 484: Reward = 246.0\n",
      "Episode 485: Reward = 207.0\n",
      "Episode 486: Reward = 210.0\n",
      "Episode 487: Reward = 358.0\n",
      "Episode 488: Reward = 223.0\n",
      "Episode 489: Reward = 257.0\n",
      "Episode 490: Reward = 271.0\n",
      "Episode 491: Reward = 244.0\n",
      "Episode 492: Reward = 226.0\n",
      "Episode 493: Reward = 240.0\n",
      "Episode 494: Reward = 208.0\n",
      "Episode 495: Reward = 373.0\n",
      "Episode 496: Reward = 288.0\n",
      "Episode 497: Reward = 328.0\n",
      "Episode 498: Reward = 223.0\n",
      "Episode 499: Reward = 229.0\n",
      "Episode 500: Reward = 233.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Define the Deep Q-Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Define the Deep Q-Learning Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99, lr=0.001, batch_size=64, memory_size=10000):\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.model = DQN(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def choose_action(self, state, epsilon=0.1):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(self.action_dim)  # Random action (exploration)\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(self.model(torch.FloatTensor(state))).item()  # Greedy action\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return  # Not enough experiences to train\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.model(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        loss = self.criterion(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "# Training Loop\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "agent = DQNAgent(state_dim=4, action_dim=2)\n",
    "\n",
    "num_episodes = 500\n",
    "epsilon = 1.0  # Initial exploration rate\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.01\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choose_action(state, epsilon)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        agent.store_experience(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        agent.train()\n",
    "\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)  # Decay epsilon\n",
    "    print(f\"Episode {episode+1}: Reward = {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948ee88d-4105-408a-b848-4321bbf8f5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
